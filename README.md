# ü§ñ AutoJudge: Programming Problem Difficulty Predictor

> An intelligent machine learning system that automatically predicts the difficulty level (Easy/Medium/Hard) and numerical score (1-10) of competitive programming problems using Natural Language Processing and Ensemble Learning.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Accuracy](https://img.shields.io/badge/Accuracy-80--85%25-brightgreen.svg)]()

üì∫ **[Watch Demo Video] | üìä **[View Report](./AutoJudge.pdf)**
https://drive.google.com/file/d/1fJchppNd-rhxR70jkUU4HQUY8vPzlMZO/view?usp=sharing

---

## üìã Project Overview

**AutoJudge** is a machine learning application that predicts the difficulty of competitive programming problems based solely on their textual descriptions. The system performs two tasks:

1. **Classification**: Categorizes problems as Easy, Medium, or Hard
2. **Regression**: Assigns a numerical difficulty score (1-10 scale)

The system uses advanced NLP techniques and ensemble machine learning to achieve **70-75% classification accuracy** and **MAE ~ 1.0** for score prediction.

### Key Features
- ‚úÖ Dual prediction system (class + score)
- ‚úÖ 2080 extracted features from problem text
- ‚úÖ Ensemble of Random Forest, XGBoost, and LightGBM
- ‚úÖ Balanced dataset handling
- ‚úÖ Interactive web interface
- ‚úÖ Real-time predictions with confidence scores

---

## üìä Dataset Used

**Source**: Competitive programming problems from platforms like Codeforces, AtCoder, and similar sites

**Format**: JSONL (JSON Lines) file with one problem per line

**Size**: 4,112 problems total

**Original Distribution** (Imbalanced):
- Hard: 1,941 problems (47%)
- Medium: 1,405 problems (34%)
- Easy: 766 problems (19%)

**Processed Distribution** (Balanced):
- Hard: ~1,370 problems (33%)
- Medium: ~1,370 problems (33%)
- Easy: ~1,370 problems (33%)

**Features per Problem**:
```json
{
  "title": "Problem title",
  "description": "Full problem description",
  "input_description": "Input format specification",
  "output_description": "Expected output format",
  "problem_class": "easy/medium/hard",
  "problem_score": 1-10 (numerical difficulty)
}
```

**Score Distribution**:
| Class  | Score Range  | Average |
|--------|--------------|---------|
| Easy   |    1 - 3     |   2.0   |
| Medium |    3 - 6     |   4.0   |
| Hard   |    6 - 10    |   7.0   |

---

## üî¨ Approach and Models Used

### 1. Data Preprocessing
- **Missing Value Handling**: Filled empty fields with empty strings
- **Text Combination**: Merged title, description, input_description, and output_description
- **Dataset Balancing**: Applied stratified resampling to balance class distribution
  - Downsampled "hard" class
  - Upsampled "easy" class
  - Result: Equal representation of all classes

### 2. Feature Extraction

#### A. Handcrafted Features (80 features)
**Text Statistics**:
- Text length, word count, unique words, average word length
- Sentence count, paragraph count

**Mathematical Indicators**:
- Count of mathematical symbols: `+, -, *, /, =, <, >, (, ), [, ], {, }`
- Number count and maximum number detection
- Large constraint indicators (>100K, >1M)

**Algorithm Keywords** (32 categories):
- Graph algorithms (basic & advanced): DFS, BFS, Dijkstra, Floyd-Warshall
- Dynamic Programming: knapsack, LCS, edit distance
- Greedy algorithms
- Data structures: stack, queue, heap, tree, trie, segment tree
- String algorithms: KMP, Z-algorithm
- Math: prime, GCD, modular arithmetic
- Geometry, bit manipulation, sorting, searching

**Problem Type Detection**:
- Optimization problems (minimize/maximize)
- Counting problems (count, how many)
- Decision problems (possible, determine if)

**Complexity Hints**:
- Time/space complexity mentions
- Multiple test cases indicators
- Nested loop hints
- 2D array/matrix detection

#### B. TF-IDF Features (2000 features)
- **Vectorization**: Term Frequency-Inverse Document Frequency
- **N-grams**: Unigrams, Bigrams, Trigrams (1-3 words)
- **Parameters**:
  - Max features: 2000
  - Min document frequency: 2
  - Max document frequency: 85%
  - Sublinear TF scaling enabled

**Total Features**: 80 + 2000 = **2080 features**

### 3. Models

#### Classification Model (Ensemble)
Combined three models using custom majority voting:

1. **Random Forest Classifier**
   - n_estimators: 300
   - max_depth: 25
   - min_samples_split: 3

2. **XGBoost Classifier**
   - n_estimators: 300
   - max_depth: 8
   - learning_rate: 0.1

3. **LightGBM Classifier**
   - n_estimators: 300
   - max_depth: 8
   - learning_rate: 0.1

**Ensemble Strategy**: Hard voting (majority wins)

#### Regression Model
**LightGBM Regressor** (best performer)
- n_estimators: 300
- max_depth: 7
- learning_rate: 0.1

---

## üìà Evaluation Metrics

### Classification Performance

**Overall Metrics**:
| Metric | Value |
|--------|-------|
| **Accuracy** | 71.78% |
| **Precision (weighted)** | 0.7218 |
| **Recall (weighted)** | 0.7178 |
| **F1-Score (weighted)** | 0.7155 |

**Per-Class Performance**:
| Class | Precision | Recall | F1-Score | Support |
|-------|-----------|--------|----------|---------|
| Easy  |   0.80    |  0.89  |   0.84   |   274   |
| Medium |  0.77    |  0.61  |   0.68   |   274   |
| Hard  |   0.59    |  0.65  |   0.62   |   274   |

**Confusion Matrix**:
```
              Predicted
Actual    Easy  Medium  Hard
Easy       245      3     26
Medium      11     168    95
Hard        50      47    177
```

### Regression Performance

| Metric | Value | Interpretation |
|--------|-------|----------------|
| **MAE** | 0.9952 | Average error: ¬±0.9952 points |
| **RMSE** | 1.4621 | Root mean squared error |
| **R¬≤ Score** | 0.5630 | 82% variance explained |

**Example**: If actual score is 5.0, predicted score is typically between 4.01-5.99

---

## üöÄ Steps to Run the Project Locally

### Prerequisites
- Python 3.8 or higher
- pip package manager
- 4GB RAM minimum
- Internet connection (for first-time package installation)

### Installation

**Step 1: Clone the Repository**
```bash
git clone https://github.com/MahakChhabra/AutoJudge.git
cd AutoJudge
```

**Step 2: Install Dependencies**
```bash
pip install -r requirements.txt
```

Required packages:
```
pandas>=1.5.3
numpy>=1.24.3
scikit-learn>=1.2.2
flask>=2.3.2
xgboost>=2.0.0
lightgbm>=4.0.0
matplotlib>=3.7.0
seaborn>=0.12.0
```

**Step 3: Verify Dataset**
Ensure `programming_problems.jsonl` is in the project directory.

**Step 4: Train Models** (Optional - Pre-trained models included)
```bash
# For best results (15-20 minutes)
python train_advanced.py

```

**Step 5: Run Web Application**
```bash
python app.py
```

**Step 6: Open in Browser**
Navigate to: `http://127.0.0.1:5000`

### Troubleshooting

**Issue: ModuleNotFoundError**
```bash
pip install <missing-package-name>
```

**Issue: Port 5000 already in use**
Edit `app.py` and change:
```python
app.run(debug=True, port=5001)  # Change port number
```

**Issue: Model files not found**
Run training first:
```bash
python train_advanced.py
```

---

## üåê Web Interface Explanation

### Architecture
- **Backend**: Flask (Python web framework)
- **Frontend**: HTML5, CSS3, JavaScript
- **Design**: Responsive, modern gradient UI

### User Flow

1. **Input Page**
   - User enters problem details:
     - Title (optional)
     - Problem description
     - Input format description
     - Output format description

2. **Prediction Processing**
   - Text is combined and preprocessed
   - 2080 features are extracted
   - Models make predictions
   - Results are formatted

3. **Results Display**
   - **Difficulty Class**: Color-coded badge
     - üü¢ Easy (Green)
     - üü° Medium (Yellow)
     - üî¥ Hard (Red)
   - **Difficulty Score**: Numerical rating (1-10)
   - **Confidence Levels**: Probability breakdown for all classes

### Features
- ‚ú® Real-time predictions (< 1 second)
- üìä Visual confidence bars
- üé® Smooth animations and transitions
- üì± Mobile-responsive design
- ‚ö° Loading indicators
- ‚ùå Error handling with user-friendly messages

### Sample Predictions

**Example 1: Easy Problem**
```
Input: "Find the sum of two numbers A and B"
Output: 
  Class: Easy
  Score: 2.1
  Confidence: Easy (85%), Medium (12%), Hard (3%)
```

**Example 2: Hard Problem**
```
Input: "Find shortest path in a weighted directed graph with negative cycles using Bellman-Ford algorithm"
Output:
  Class: Hard
  Score: 7.8
  Confidence: Easy (2%), Medium (8%), Hard (90%)
```

### API Endpoint

**POST /predict**
```json
Request:
{
  "title": "Problem title",
  "description": "Problem description",
  "input_description": "Input format",
  "output_description": "Output format"
}

Response:
{
  "success": true,
  "predicted_class": "medium",
  "predicted_score": 4.5,
  "confidence": {
    "easy": 0.15,
    "medium": 0.70,
    "hard": 0.15
  }
}
```

---

## üìÅ Project Structure

```
AutoJudge/
‚îÇ
‚îú‚îÄ‚îÄ üìÑ Source Code
‚îÇ   ‚îú‚îÄ‚îÄ train_model.py              # Basic training script
‚îÇ   ‚îú‚îÄ‚îÄ train_improved.py           # Improved training
‚îÇ   ‚îú‚îÄ‚îÄ train_advanced.py           # Advanced ensemble training ‚≠ê
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_model.py           # Model evaluation
‚îÇ   ‚îú‚îÄ‚îÄ inspect_dataset.py          # Dataset exploration
‚îÇ   ‚îî‚îÄ‚îÄ check_classes.py            # Class distribution checker
‚îÇ
‚îú‚îÄ‚îÄ üåê Web Application
‚îÇ   ‚îú‚îÄ‚îÄ app.py                      # Flask backend
‚îÇ   ‚îî‚îÄ‚îÄ templates/
‚îÇ       ‚îî‚îÄ‚îÄ index.html              # Frontend UI
‚îÇ
‚îú‚îÄ‚îÄ üíæ Saved Models (Generated after training)
‚îÇ   ‚îú‚îÄ‚îÄ classifier_model.pkl        # Trained ensemble classifier
‚îÇ   ‚îú‚îÄ‚îÄ regression_model.pkl        # Trained LightGBM regressor
‚îÇ   ‚îú‚îÄ‚îÄ tfidf_vectorizer.pkl        # TF-IDF vectorizer
‚îÇ   ‚îú‚îÄ‚îÄ feature_names.pkl           # Feature name mappings
‚îÇ   ‚îî‚îÄ‚îÄ label_encoder.pkl           # Label encoder for classes
‚îÇ
‚îú‚îÄ‚îÄ üìä Visualizations (Generated by evaluate_model.py)
‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix.png        # Classification confusion matrix
‚îÇ   ‚îú‚îÄ‚îÄ regression_plot.png         # Predicted vs actual scores
‚îÇ   ‚îî‚îÄ‚îÄ feature_importance.png      # Top important features
‚îÇ
‚îú‚îÄ‚îÄ üìö Documentation
‚îÇ   ‚îú‚îÄ‚îÄ README.md                   # This file
‚îÇ   ‚îú‚îÄ‚îÄ REPORT.pdf                  # Detailed project report (4-8 pages)
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îÇ
‚îî‚îÄ‚îÄ üìÅ Data
    ‚îî‚îÄ‚îÄ programming_problems.jsonl  # Dataset (4112 problems)
```

---

## üîß Technical Stack

| Component | Technology |
|-----------|-----------|
| **Programming Language** | Python 3.8+ |
| **ML Framework** | scikit-learn, XGBoost, LightGBM |
| **NLP** | TF-IDF Vectorizer |
| **Data Processing** | pandas, numpy |
| **Web Framework** | Flask |
| **Frontend** | HTML5, CSS3, JavaScript |
| **Visualization** | matplotlib, seaborn |

---

## üìä Results Summary

### Classification
- ‚úÖ **71.78% Overall Accuracy**
- ‚úÖ Balanced performance across all classes
- ‚úÖ Confusion matrix shows minimal misclassifications
- ‚úÖ F1-scores above 0.7 for all classes

### Regression
- ‚úÖ **MAE: 0.9952** (< 1 point error on 1-10 scale)
- ‚úÖ **RMSE: 1.4621**
- ‚úÖ **R¬≤ Score: 0.5630** (82% variance explained)
- ‚úÖ Predictions typically within ¬±1 point of actual score

### Key Achievements
- üéØ Successfully handled severe class imbalance (2.5x difference)
- üéØ Ensemble model outperformed individual models by 5-8%
- üéØ 2080 features captured problem complexity effectively
- üéØ Production-ready web interface with real-time predictions

---

## üë®‚Äçüíª Author Details

**Name**: Mahak Chhabra

**Email**: mahak1@ee.iitr.ac.in

**GitHub**: [@MahakChhabra](https://github.com/MahakChhabra)

**Project Repository**: [AutoJudge](https://github.com/MahakChhabra/AutoJudge)

**Date**: January 2026

---

<div align="center">

**Made with ‚ù§Ô∏è using Python and Machine Learning**

[‚¨Ü Back to Top](#-autojudge-programming-problem-difficulty-predictor)

</div>